{
  "json_schema_major_version": "1",
  "json_schema_minor_version": "2",
  "json_content_version": "1",
  "function_name": "TextTokenizer",
  "function_version": "3.3",
  "function_type": "non-driver",
  "function_r_name": "aa.text.tokenizer",
  "short_description": "Text classification SQL/MR function",
  "long_description": "Predict the category of the document in each text column",
  "input_tables": [
    {
      "isOrdered": false,
      "useInR": true,
      "name": "input",
      "rName": "data",
      "isRequired": true,
      "datatype": "TABLE_ALIAS",
      "rOrderNum": 1,
      "allowsLists": false,
      "partitionByOne": false,
      "rDescription": "The relation contains the text to be scanned",
      "requiredInputKind": [
        "PartitionByAny"
      ],
      "alternateNames": [],
      "description": "The relation contains the text to be scanned"
    },
    {
      "isOrdered": false,
      "useInR": true,
      "name": "dict",
      "rName": "dict.data",
      "isRequired": false,
      "datatype": "TABLE_ALIAS",
      "rOrderNum": 2,
      "allowsLists": false,
      "partitionByOne": false,
      "rDescription": "The relation that contains the dictionary for segementing words.",
      "requiredInputKind": [
        "Dimension"
      ],
      "alternateNames": [],
      "description": "The relation that contains the dictionary for segementing words."
    }
  ],
  "argument_clauses": [
    {
      "name": "TextColumn",
      "allowsLists": false,
      "useInR": true,
      "rName": "text.column",
      "allowedTypes": [],
      "datatype": "COLUMNS",
      "targetTable": [
        "input"
      ],
      "rOrderNum": 3,
      "checkDuplicate": true,
      "isRequired": true,
      "rDescription": "Specifies the name of the input table column that contains the text to tokenize",
      "requiredLength": 1,
      "alternateNames": [],
      "allowedTypeGroups": [
        "ALL"
      ],
      "allowPadding": true,
      "matchLengthOfArgument": "",
      "description": "Specifies the name of the input table column that contains the text to tokenize"
    },
    {
      "name": "Accumulate",
      "allowsLists": true,
      "useInR": true,
      "rName": "accumulate",
      "allowedTypes": [],
      "datatype": "COLUMNS",
      "targetTable": [
        "input"
      ],
      "rOrderNum": 9,
      "checkDuplicate": true,
      "isRequired": false,
      "rDescription": " Specifies the names of the input table columns to copy to the output table.",
      "alternateNames": [],
      "allowedTypeGroups": [
        "ALL"
      ],
      "allowPadding": true,
      "matchLengthOfArgument": "",
      "description": " Specifies the names of the input table columns to copy to the output table."
    },
    {
      "name": "Language",
      "allowsLists": false,
      "isOutputColumn": false,
      "useInR": true,
      "datatype": "STRING",
      "defaultValue": "en",
      "rOrderNum": 4,
      "rName": "language",
      "isRequired": false,
      "rDescription": "Specifies the language of the text in text_column:  en (English, the default),  zh_CN (Simplified Chinese),  zh_TW (Traditional Chinese),  jp (Japanese)",
      "alternateNames": [],
      "permittedValues": [
        "en",
        "zh_CN",
        "zh_TW",
        "jp"
      ],
      "description": "Specifies the language of the text in text_column:  en (English, the default),  zh_CN (Simplified Chinese),  zh_TW (Traditional Chinese),  jp (Japanese)"
    },
    {
      "name": "OutputDelimiter",
      "allowsLists": false,
      "isOutputColumn": false,
      "useInR": true,
      "datatype": "STRING",
      "defaultValue": "/",
      "rOrderNum": 6,
      "rName": "output.delimiter",
      "isRequired": false,
      "rDescription": "Specifies the delimiter for separating tokens in the output. The default value is slash (/).",
      "alternateNames": [],
      "permittedValues": [],
      "description": "Specifies the delimiter for separating tokens in the output. The default value is slash (/)."
    },
    {
      "name": "OutputByWord",
      "allowsLists": false,
      "useInR": true,
      "datatype": "BOOLEAN",
      "defaultValue": false,
      "rOrderNum": 7,
      "rName": "output.byword ",
      "isRequired": false,
      "rDescription": "Specifies whether to output one token in each row. The default value is 'false' (output one line of text in each row).",
      "alternateNames": [],
      "description": "Specifies whether to output one token in each row. The default value is 'false' (output one line of text in each row)."
    },
    {
      "name": "userDictionaryFile",
      "allowsLists": false,
      "isOutputColumn": false,
      "useInR": true,
      "datatype": "STRING",
      "rOrderNum": 8,
      "rName": "user.dictionary",
      "isRequired": false,
      "rDescription": "Specifies the name of the user dictionary to use to correct results specified by the model. If you specify both this argument and a dictionary table (dict), then the function uses the union of user_dictionary_file and dict as its dictionary. That describes the format of user_dictionary_file and dict. Note: If the function finds more than one matched term, it selects the longest term for the first match.",
      "alternateNames": [],
      "permittedValues": [],
      "description": "Specifies the name of the user dictionary to use to correct results specified by the model. If you specify both this argument and a dictionary table (dict), then the function uses the union of user_dictionary_file and dict as its dictionary. That describes the format of user_dictionary_file and dict. Note: If the function finds more than one matched term, it selects the longest term for the first match."
    },
    {
      "name": "Model",
      "allowsLists": false,
      "isOutputColumn": false,
      "useInR": true,
      "datatype": "STRING",
      "rOrderNum": 5,
      "rName": "model",
      "isRequired": false,
      "rDescription": "Specifies the name of model file that the function uses for tokenizing. The model must be a conditional random-fields model and model_file must already be installed on the database. If you omit this argument, or if model_file is not installed on the database, then the function uses white spaces to separate English words and an embedded dictionary to tokenize Chinese text. Note: If you specify Language('jp'), the function ignores this argument.",
      "alternateNames": [],
      "permittedValues": [],
      "description": "Specifies the name of model file that the function uses for tokenizing. The model must be a conditional random-fields model and model_file must already be installed on the database. If you omit this argument, or if model_file is not installed on the database, then the function uses white spaces to separate English words and an embedded dictionary to tokenize Chinese text. Note: If you specify Language('jp'), the function ignores this argument."
    }
  ]
}